{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import deep_laa_support as dls\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# read data\n",
    "# filename = \"feature_31_L\"\n",
    "data_all = np.load(filename +'.npz')\n",
    "print('file ' + filename + '.npz ' 'loaded.')\n",
    "user_labels = data_all['user_labels']\n",
    "true_labels = data_all['true_labels']\n",
    "category_size = data_all['category_num']\n",
    "source_num = data_all['source_num']\n",
    "feature = data_all['feature']\n",
    "_, feature_size = np.shape(feature)\n",
    "n_samples, _ = np.shape(true_labels)\n",
    "\n",
    "#================= basic parameters =====================\n",
    "# define batch size (use all samples in one batch)\n",
    "batch_size = n_samples # n_samples\n",
    "cluster_num = 200\n",
    "T = 1 # mc_sampling_times\n",
    "\n",
    "#================= encoder q(y|l) and q(h|x) =====================\n",
    "with tf.name_scope('encoder'):\n",
    "    #================= q(y|l) =====================\n",
    "    # define input l (source label vectors)\n",
    "    input_size = source_num * category_size\n",
    "    with tf.variable_scope('q_yl'):\n",
    "        l = tf.placeholder(dtype=tf.float32, shape=[batch_size, input_size], name='l_input')\n",
    "        pi_yl, weights_yl, biases_yl = dls.LAA_encoder(l, batch_size, source_num, category_size)\n",
    "    # loss: cross entropy between y_classifier and y_target for pre-training classifier\n",
    "    with tf.variable_scope('q_yl'):\n",
    "        pi_yl_target = tf.placeholder(dtype=tf.float32, shape=[batch_size, category_size], name='pi_yl_target')\n",
    "        loss_yl = dls.LAA_loss_classifier(pi_yl, pi_yl_target)\n",
    "    # optimizier\n",
    "    learning_rate_yl = 0.01\n",
    "    optimizer_pre_train_yl = tf.train.AdamOptimizer(learning_rate=learning_rate_yl).minimize(loss_yl)\n",
    "\n",
    "    #================= q(h|x) =====================\n",
    "    h1_size_encoder = int(np.floor(feature_size/2.0))\n",
    "    h2_size_encoder = int(np.floor(feature_size/4.0))\n",
    "    embedding_size = int(np.floor(feature_size/8.0))\n",
    "    h1_size_decoder = int(np.floor(feature_size/4.0))\n",
    "    h2_size_decoder = int(np.floor(feature_size/2.0))\n",
    "    with tf.variable_scope('q_hx'):\n",
    "        x = tf.placeholder(dtype=tf.float32, shape=[batch_size, feature_size], name='x_input')\n",
    "        # mu_hx[batch_size, embedding_size]\n",
    "        # sigma_hx[batch_size, embedding_size]\n",
    "        with tf.variable_scope('feature_encoder_h1'):\n",
    "            _h1_encoder, w1_encoder, b1_encoder = dls.full_connect_relu_BN(x, [feature_size, h1_size_encoder])\n",
    "        with tf.variable_scope('feature_encoder_h2'):\n",
    "            _h2_encoder, w2_encoder, b2_encoder = dls.full_connect_relu_BN(_h1_encoder, [h1_size_encoder, h2_size_encoder])\n",
    "        with tf.variable_scope('feature_encoder_mu'):\n",
    "            mu_hx, w_mu_encoder, b_mu_encoder = dls.full_connect(_h2_encoder, [h2_size_encoder, embedding_size])\n",
    "        with tf.variable_scope('feature_encoder_sigma'):\n",
    "            sigma_hx, w_sigma_encoder, b_sigma_encoder = dls.full_connect(_h2_encoder, [h2_size_encoder, embedding_size])\n",
    "        # mu_hx, sigma_hx = dls.vae_encoder(x, feature_size, h1_size_encoder, h2_size_encoder, embedding_size)\n",
    "        # embedding_h[batch_size, T, embedding_size]\n",
    "        embedding_h = tf.reshape(mu_hx, [batch_size, 1, embedding_size]) \\\n",
    "            + tf.reshape(sigma_hx, [batch_size, 1, -1]) \\\n",
    "            * tf.random_normal(shape=[batch_size, T, embedding_size], mean=0, stddev=1, dtype=tf.float32)\n",
    "\n",
    "    with tf.variable_scope('q_hx_AE'):\n",
    "        # x_reconstr, _, _ = dls.vae_decoder(mu_hx, embedding_size, h1_size_decoder, h2_size_decoder, feature_size)\n",
    "        with tf.variable_scope('feature_decoder_h1'):\n",
    "            _h1_decoder, w1_decoder, b1_decoder = dls.full_connect_relu_BN(mu_hx, [embedding_size, h1_size_decoder])\n",
    "        with tf.variable_scope('feature_decoder_h2'):\n",
    "            _h2_decoder, w2_decoder, b2_decoder = dls.full_connect_relu_BN(_h1_decoder, [h1_size_decoder, h2_size_decoder])\n",
    "        with tf.variable_scope('feature_decoder_rho'):\n",
    "            x_reconstr, w_rho_decoder, b_rho_decoder = dls.full_connect_sigmoid(_h2_decoder, [h2_size_decoder, feature_size])\n",
    "\n",
    "        loss_cross_entropy_AE = -tf.reduce_mean(tf.reduce_sum(x*tf.log(1e-10+x_reconstr) + (1.0-x)*tf.log(1e-10+(1.0-x_reconstr)), -1))\n",
    "        _loss_square_AE = tf.reduce_mean(tf.square(x_reconstr - x))\n",
    "        constraint_w_AE = 0.5 * (tf.reduce_mean(tf.square(w1_encoder)) + tf.reduce_mean(tf.square(b1_encoder))\n",
    "            + tf.reduce_mean(tf.square(w2_encoder)) + tf.reduce_mean(tf.square(b2_encoder))\n",
    "            + tf.reduce_mean(tf.square(w_mu_encoder)) + tf.reduce_mean(tf.square(b_mu_encoder))\n",
    "            + tf.reduce_mean(tf.square(w1_decoder)) + tf.reduce_mean(tf.square(b1_decoder))\n",
    "            + tf.reduce_mean(tf.square(w2_decoder)) + tf.reduce_mean(tf.square(b2_decoder))\n",
    "            + tf.reduce_mean(tf.square(w_rho_decoder)) + tf.reduce_mean(tf.square(b_rho_decoder)))\n",
    "        loss_AE = loss_cross_entropy_AE \\\n",
    "            + constraint_w_AE\n",
    "        learning_rate_AE = 0.02\n",
    "        optimizer_AE = tf.train.AdamOptimizer(learning_rate=learning_rate_AE).minimize(loss_AE)\n",
    "\n",
    "    #================= p(x|h) =====================\n",
    "    with tf.variable_scope('q_hx_AE'):\n",
    "        with tf.variable_scope('feature_decoder_h1', reuse=True):\n",
    "            _h_VAE = tf.reshape(embedding_h, [-1, embedding_size])\n",
    "            _h1_decoder_VAE, _, _ = dls.full_connect_relu_BN(_h_VAE, [embedding_size, h1_size_decoder])\n",
    "        with tf.variable_scope('feature_decoder_h2', reuse=True):\n",
    "            _h2_decoder_VAE, _, _ = dls.full_connect_relu_BN(_h1_decoder_VAE, [h1_size_decoder, h2_size_decoder])\n",
    "        with tf.variable_scope('feature_decoder_rho', reuse=True):\n",
    "            mu_xh, _, _ = dls.full_connect_sigmoid(_h2_decoder_VAE, [h2_size_decoder, feature_size])\n",
    "            mu_xh = tf.reshape(mu_xh, [batch_size, T, feature_size])\n",
    "\n",
    "    print('Encoders are constructed.')\n",
    "    \n",
    "#================= decoder p(l|y), p(x|h), p(y|z), p(h|z) and p(z) =====================\n",
    "with tf.name_scope('decoder'):\n",
    "    #================= p(l|y) =====================\n",
    "    with tf.variable_scope('p_ly'):\n",
    "        # pi_ly[category_size, 1, source_num*category_size]\n",
    "        pi_ly, weights_ly, biases_ly = dls.LAA_decoder(source_num, category_size)\n",
    "\n",
    "        constraint_w_LAA = 0.5 * (tf.reduce_mean(tf.square(weights_ly)) + tf.reduce_mean(tf.square(biases_ly))\n",
    "            + tf.reduce_mean(tf.square(weights_yl)) + tf.reduce_mean(tf.square(biases_yl)))\n",
    "        \n",
    "    #================= p(y|z) =====================\n",
    "    with tf.variable_scope('p_yz'):\n",
    "        # pi_yz[cluster_num, category_size]\n",
    "        _pi_yz = tf.get_variable('pi_yz', dtype=tf.float32, \n",
    "                                initializer=tf.random_normal(shape=[cluster_num, category_size], mean=0, stddev=1, dtype=tf.float32))\n",
    "        __pi_yz = tf.exp(_pi_yz)\n",
    "        pi_yz = tf.div(__pi_yz, tf.reduce_sum(__pi_yz, -1, keepdims=True))\n",
    "        \n",
    "        pi_yz_assign = tf.placeholder(dtype=tf.float32, shape=[cluster_num, category_size], name='pi_yz_assign')\n",
    "        initialize_pi_yz = tf.assign(_pi_yz, pi_yz_assign)\n",
    "        \n",
    "    #================= p(h|z) =====================\n",
    "    with tf.variable_scope('p_hz'):\n",
    "        # mu_hz[cluster_num, embedding_size]\n",
    "        # sigma_hz[cluster_num, embedding_size]\n",
    "        mu_hz = tf.get_variable('mu_hz', dtype=tf.float32, initializer=tf.random_normal(shape=[cluster_num, embedding_size], mean=0, stddev=1, dtype=tf.float32))\n",
    "        sigma_hz = tf.get_variable('sigma_hz', dtype=tf.float32, initializer=tf.ones([cluster_num, embedding_size], dtype=tf.float32))\n",
    "\n",
    "        mu_hz_assign = tf.placeholder(dtype=tf.float32, shape=[cluster_num, embedding_size], name='mu_hz_assign')\n",
    "        initialize_mu_hz = tf.assign(mu_hz, mu_hz_assign)\n",
    "        \n",
    "    #================= p(z) =====================\n",
    "    with tf.variable_scope('p_z'):\n",
    "        # pi_z_prior[batch_size, cluster_num]\n",
    "        pi_z_prior = tf.placeholder(dtype=tf.float32, shape=[batch_size, cluster_num], name='pi_z_prior')\n",
    "        _pi_z = tf.get_variable('pi_z', dtype=tf.float32, initializer=tf.ones([batch_size, cluster_num]))\n",
    "        __pi_z = tf.exp(_pi_z)\n",
    "        pi_z = tf.div(__pi_z, tf.reduce_sum(__pi_z, -1, keepdims=True))\n",
    "\n",
    "        pi_z_assign = tf.placeholder(dtype=tf.float32, shape=[batch_size, cluster_num], name='pi_z_assign')\n",
    "        initialize_pi_z = tf.assign(_pi_z, pi_z_assign)\n",
    "    print('Decoders are constructed.')\n",
    "    \n",
    "#================= elbo =====================\n",
    "'''\n",
    "q(h|x) log p(x|h)\n",
    "q(y|l) log p(l|y)\n",
    "q(h|x) log q(h|x)\n",
    "q(y|l) log q(y|l)\n",
    "q(z|x,l)q(h|x) log p(h|z)\n",
    "q(z|x,l)q(y|l) log p(y|z)\n",
    "q(z|x,l) log p(z)\n",
    "q(z|x,l) log q(z|x,l)\n",
    "q(z|x,l)\n",
    "'''\n",
    "with tf.name_scope('elbo'):\n",
    "    #================= q(h|x) log p(x|h) =====================\n",
    "    with tf.name_scope('q_hx_log_p_xh'):\n",
    "        # reduce_mean along both T and batch_size\n",
    "        _tmp = tf.reshape(x, [batch_size, 1, feature_size])\n",
    "        elbo_q_hx_log_p_xh = tf.reduce_mean(tf.reduce_sum(_tmp*tf.log(1e-10+mu_xh) + (1.0-_tmp)*tf.log(1e-10+(1.0-mu_xh)), -1))\n",
    "    \n",
    "    #================= q(y|l) log p(l|y) =====================\n",
    "    with tf.name_scope('q_yl_log_p_ly'):\n",
    "        elbo_q_yl_log_p_ly = -dls.LAA_loss_reconstr(l, pi_ly, pi_yl)\n",
    "        \n",
    "    #================= q(h|x) log q(h|x) =====================\n",
    "    with tf.name_scope('q_hx_log_q_hx'):\n",
    "        elbo_q_hx_log_q_hx = -0.5 * tf.reduce_mean(tf.reduce_sum(tf.log(1e-10+tf.square(sigma_hx)), -1))\n",
    "\n",
    "    #================= q(y|l) log q(y|l) =====================\n",
    "    with tf.name_scope('q_yl_log_q_yl'):\n",
    "        elbo_q_yl_log_q_yl = tf.reduce_mean(tf.reduce_sum(pi_yl * tf.log(1e-10+pi_yl), -1))\n",
    "    \n",
    "    #================= q(z|x,l) =====================\n",
    "    with tf.name_scope('q_zxl'):\n",
    "        # p(h|z)[batch_size, T, cluster_num, 1]\n",
    "        _h = tf.reshape(embedding_h, [batch_size, T, 1, embedding_size])\n",
    "        _p_hz = -0.5 * tf.reduce_sum(\n",
    "            tf.div(tf.square(_h-mu_hz), 1e-10+tf.square(sigma_hz)) \n",
    "            + tf.log(1e-10 + tf.square(sigma_hz)), -1, keepdims=True)\n",
    "        # p_zhy[batch_size, T, cluster_num, category_size]\n",
    "        _p_zhy = tf.log(1e-10+pi_yz) + _p_hz + tf.log(1e-10+tf.reshape(pi_z, [batch_size, 1, cluster_num, 1]))\n",
    "        _p_zhy_max = tf.reduce_max(_p_zhy, 2, keepdims=True)\n",
    "        p_zhy = tf.exp(_p_zhy - (_p_zhy_max + tf.log(1e-10+tf.reduce_sum(tf.exp(_p_zhy-_p_zhy_max), 2, keepdims=True))))\n",
    "        \n",
    "        # q_zxl[batch_size, cluster_num]\n",
    "        # reduce_mean along both category_size and T\n",
    "        _q_zxl = tf.reduce_sum(tf.reshape(pi_yl, [batch_size, 1, 1, category_size]) * p_zhy, -1)\n",
    "        q_zxl = tf.reduce_mean(_q_zxl, 1)\n",
    "        \n",
    "        # z_index[batch_size]\n",
    "        z_index = tf.argmax(q_zxl, 1)\n",
    "        # cluster_pi_max[batch_size, category_size]\n",
    "        # cluster_pi_avg[batch_size, category_size]\n",
    "        cluster_pi_max = tf.gather(pi_yz, z_index)\n",
    "        cluster_pi_avg = tf.matmul(q_zxl, pi_yz)\n",
    "        \n",
    "    #================= q(z|x,l)q(h|x) log p(h|z) =====================\n",
    "    #================= q(h|x) log p(h|z) [batch_size, cluster_num] =====================\n",
    "    with tf.name_scope('q_zxl_q_hx_log_p_hz'):\n",
    "        # mu_hx[batch_size, embedding_size]\n",
    "        # sigma_hx[batch_size, embedding_size]\n",
    "        # mu_hz[cluster_num, embedding_size]\n",
    "        # sigma_hz[cluster_num, embedding_size]\n",
    "        _part_1 = tf.div(tf.square(tf.reshape(mu_hx, [batch_size, 1, embedding_size]) - mu_hz), 1e-10+tf.square(sigma_hz))\n",
    "        _part_2 = tf.div(tf.square(tf.reshape(sigma_hx, [batch_size, 1, -1])), 1e-10+tf.square(sigma_hz))\n",
    "        _part_3 = tf.log(1e-10 + tf.square(sigma_hz))\n",
    "        # elbo_q_hx_log_p_hz[batch_size, cluster_num]\n",
    "        elbo_q_hx_log_p_hz = -0.5 * tf.reduce_sum(_part_1 + _part_2 + _part_3, -1)\n",
    "        elbo_q_zxl_q_hx_log_p_hz = tf.reduce_mean(tf.reduce_sum(q_zxl * elbo_q_hx_log_p_hz, -1))\n",
    "    \n",
    "    #================= q(z|x,l)q(y|l) log p(y|z) =====================\n",
    "    #================= q(y|l) log p(y|z) [batch_size, cluster_num] =====================\n",
    "    with tf.name_scope('q_zxl_q_yl_log_p_yz'):\n",
    "        # pi_yz[cluster_num, category_size]\n",
    "        # pi_yl[batch_size, category_size]\n",
    "        # elbo_q_yl_log_p_yz[batch_size, cluster_num]\n",
    "        elbo_q_yl_log_p_yz = tf.reduce_sum(tf.reshape(pi_yl, [batch_size, 1, category_size]) * tf.log(1e-10 + pi_yz), -1)\n",
    "        elbo_q_zxl_q_yl_log_p_yz = tf.reduce_mean(tf.reduce_sum(q_zxl * elbo_q_yl_log_p_yz, -1))\n",
    "    \n",
    "    #================= q(z|x,l) log p(z) =====================\n",
    "    #================= log p(z) [cluster_num] =====================\n",
    "    with tf.name_scope('q_zxl_log_p_z'):\n",
    "        # elbo_log_p_z[batch_size, cluster_num]\n",
    "        elbo_log_p_z = tf.log(1e-10 + pi_z)\n",
    "        elbo_q_zxl_log_p_z = tf.reduce_mean(tf.reduce_sum(q_zxl * elbo_log_p_z, -1))\n",
    "    \n",
    "    #================= q(z|x,l) log q(z|x,l) =====================\n",
    "    with tf.name_scope('q_zxl_log_q_zxl'):\n",
    "        # q_zxl[batch_size, cluster_num]\n",
    "        elbo_q_zxl_log_q_zxl = tf.reduce_mean(tf.reduce_sum(q_zxl * tf.log(1e-10 + q_zxl), -1))\n",
    "    \n",
    "    #================= overall elbo =====================\n",
    "    elbo = elbo_q_hx_log_p_xh + elbo_q_yl_log_p_ly - elbo_q_hx_log_q_hx - elbo_q_yl_log_q_yl \\\n",
    "        + elbo_q_zxl_q_hx_log_p_hz + elbo_q_zxl_q_yl_log_p_yz + elbo_q_zxl_log_p_z - elbo_q_zxl_log_q_zxl    \n",
    "    \n",
    "    q_zxl_entropy = -elbo_q_zxl_log_q_zxl\n",
    "    \n",
    "    with tf.variable_scope('regularization_prior'):\n",
    "        mu_hz_prior_mu = tf.placeholder(dtype=tf.float32, shape=[cluster_num, embedding_size], name='mu_hz_prior_mu')\n",
    "        # sigma_hz_prior_alpha = tf.placeholder(dtype=tf.float32, shape=[cluster_num, embedding_size], name='sigma_hz_prior')\n",
    "        pi_yz_prior = tf.placeholder(dtype=tf.float32, shape=[cluster_num, category_size], name='pi_yz_prior')\n",
    "    \n",
    "        constraint_prior = 0.5*tf.reduce_mean(tf.square(mu_hz - mu_hz_prior_mu)) \\\n",
    "            - tf.reduce_mean(pi_yz_prior * tf.log(1e-10+pi_yz)) \\\n",
    "            - tf.reduce_mean(pi_z_prior * tf.log(1e-10+pi_z)) \\\n",
    "            + tf.reduce_mean(1.0*tf.log(1e-10+tf.square(sigma_hz))+tf.div(2.0, 1e-10+tf.square(sigma_hz)))\n",
    "    \n",
    "    loss_overall = -elbo \\\n",
    "        + constraint_w_AE \\\n",
    "        + constraint_w_LAA \\\n",
    "        + 1.0 * constraint_prior\n",
    "    \n",
    "    # optimizier\n",
    "    learning_rate_overall = 0.001\n",
    "    optimizer_overall = tf.train.AdamOptimizer(learning_rate=learning_rate_overall).minimize(loss_overall)\n",
    "\n",
    "    print('Clustering-based label-aware autoencoder is constructed.')\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#================= training and inference =====================\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #================= pre-train pi_yl =====================\n",
    "    # assign batch variables (use whole data in one batch)\n",
    "    # define majority voting regularizer\n",
    "    majority_y = dls.get_majority_y(user_labels, source_num, category_size)\n",
    "    # pre-train classifier\n",
    "    print(\"Pre-train pi_yl ...\")\n",
    "    epochs = 50\n",
    "    for epoch in range(epochs):\n",
    "        _, monitor_pi_yl = sess.run([optimizer_pre_train_yl, pi_yl], \n",
    "            feed_dict={l:user_labels, pi_yl_target:majority_y})\n",
    "        if epoch % 10 == 0:\n",
    "            hit_num = dls.cal_hit_num(true_labels, monitor_pi_yl)\n",
    "            print(\"epoch: {0} accuracy: {1}\".format(epoch, float(hit_num)/n_samples))\n",
    "    \n",
    "    print(\"Pre-train hx_AE ...\")\n",
    "    epochs = 2000\n",
    "    for epoch in range(epochs):\n",
    "        _, monitor_loss_square_AE, monitor_mu_hx = sess.run([optimizer_AE, _loss_square_AE, mu_hx], \n",
    "            feed_dict={x:feature})\n",
    "        if epoch % 50 == 0:\n",
    "            print(\"epoch: {0} loss: {1}\".format(epoch, monitor_loss_square_AE))    \n",
    "    \n",
    "    #================= calculate initial parameters =====================\n",
    "    clustering_result = KMeans(n_clusters=cluster_num).fit(monitor_mu_hx)\n",
    "    pi_z_prior_cluster = dls.convert_to_one_hot(clustering_result.labels_, cluster_num, smooth=0.2)\n",
    "    _ = sess.run(initialize_mu_hz, {mu_hz_assign:clustering_result.cluster_centers_})\n",
    "    pi_yz_prior_cluster = dls.get_cluster_majority_y(\n",
    "        clustering_result.labels_, user_labels, cluster_num, source_num, category_size)\n",
    "    _ = sess.run(initialize_pi_yz, {pi_yz_assign:pi_yz_prior_cluster})\n",
    "    _ = sess.run(initialize_pi_z, {pi_z_assign:pi_z_prior_cluster})\n",
    "    \n",
    "    mu_hz_prior_mu_cluster = clustering_result.cluster_centers_\n",
    "    \n",
    "    predict_label = np.zeros([batch_size, category_size])\n",
    "    for i in range(batch_size):\n",
    "        predict_label[i] = pi_yz_prior_cluster[clustering_result.labels_[i], :]\n",
    "    print(\"Initial clustering accuracy: {0}\".format(float(dls.cal_hit_num(true_labels, predict_label)) / n_samples))\n",
    "    \n",
    "    #================= save current model =====================\n",
    "    saved_path = saver.save(sess, './my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './my_model')\n",
    "    \n",
    "    print(\"Train overall net ...\")\n",
    "    epochs = 2000\n",
    "    for epoch in range(epochs):\n",
    "        _, monitor_loss_overall, monitor_pi_yl, monitor_cluster_pi_max, monitor_cluster_pi_avg, \\\n",
    "            monitor_constraint_w_AE, monitor_constraint_prior = sess.run(\n",
    "                [optimizer_overall, loss_overall, pi_yl, cluster_pi_max, cluster_pi_avg, constraint_w_AE, constraint_prior], \n",
    "                feed_dict={l:user_labels, x:feature, \n",
    "                           pi_z_prior:pi_z_prior_cluster, \n",
    "                           mu_hz_prior_mu:mu_hz_prior_mu_cluster, \n",
    "                           pi_yz_prior:pi_yz_prior_cluster})\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"epoch: {0} loss: {1}\".format(epoch, monitor_loss_overall))\n",
    "            print(\"epoch: {0} loss: {1}\".format(epoch, monitor_constraint_w_AE))\n",
    "            print(\"epoch: {0} loss: {1}\".format(epoch, monitor_constraint_prior))\n",
    "            hit_num_object_level = dls.cal_hit_num(true_labels, monitor_pi_yl)\n",
    "            hit_num_cluster_level_max = dls.cal_hit_num(true_labels, monitor_cluster_pi_max)\n",
    "            hit_num_cluster_level_avg = dls.cal_hit_num(true_labels, monitor_cluster_pi_avg)\n",
    "            print(\"epoch: {0} accuracy(object level): {1}\".format(epoch, float(hit_num_object_level)/n_samples))\n",
    "            print(\"epoch: {0} accuracy(cluster level max): {1}\".format(epoch, float(hit_num_cluster_level_max)/n_samples))\n",
    "            print(\"epoch: {0} accuracy(cluster level avg): {1}\".format(epoch, float(hit_num_cluster_level_avg)/n_samples))\n",
    "    print(\"Training overall net. Done!\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
